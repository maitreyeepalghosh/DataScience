import collections
import nltk
import re
import pandas as pd
from nltk import word_tokenize
from nltk.corpus import stopwords
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns
import sys
reload(sys)
sys.setdefaultencoding('utf8')

#Read from excel or csv file
dframe = pd.read_excel('C:\\Maitreyee\\Test_Data.xlsx')
sentences=dframe['Issue Description']

# Load nltk's SnowballStemmer as variabled 'stemmer'
from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer("english")

# Here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed
def tokenize_and_stem(text):
    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token
    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]
    filtered_tokens = []
    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)
    for token in tokens:
        if re.search('[a-zA-Z]', token):
            filtered_tokens.append(token)
    stems = [stemmer.stem(t) for t in filtered_tokens]
    return stems
    
 
#Generate vector from Text using TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,
                                 min_df=0.2, stop_words='english',
                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))
#builds a tf-idf matrix for the sentences
tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)


#Find optimal value of k using Elbow method
s = []

for n_clusters in range(2,30):
    print(n_clusters)
    km = KMeans(n_clusters=n_clusters)
    %time km.fit(tfidf_matrix)
    labels = km.labels_
    centroids = km.cluster_centers_
    s.append(silhouette_score(tfidf_matrix, labels, metric='euclidean'))

plt.plot(s)
plt.ylabel("Silouette")
plt.xlabel("k")
plt.title("Silouette for K-means cell's behaviour")
plt.show()
sns.despine()
plt.show()

#Generate cluster using KMeans algorithm
def cluster_sentences(sentences, nb_of_clusters=5):
       
        kmeans = KMeans(n_clusters=nb_of_clusters)
        kmeans.fit(tfidf_matrix)
        clusters = collections.defaultdict(list)
        #print(kmeans.labels_)
        for i, label in enumerate(kmeans.labels_):
            print( clusters[label])
            clusters[label].append(i)
        return dict(clusters)
        

#As we found optimal value of k is 5 do we will generate 5 clusters
nclusters= 5
clusters = cluster_sentences(sentences, nclusters)
dict1={}
for cluster in range(nclusters):
    for i,sentence in enumerate(clusters[cluster]):
        dict1[sentences[sentence]]=cluster
        
        
#Generate output excel 
df=pd.DataFrame.from_dict(dict1, orient="index")
df.to_csv("data.csv")
