{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Regression Plots\n",
    "\n",
    "from __future__ import print_function\n",
    "from statsmodels.compat import lzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Duncan's Prestige Dataset\n",
    "\n",
    "#### Load the Data\n",
    "\n",
    "# We can use a utility function to load any R dataset available from the great <a href=\"http://vincentarelbundock.github.com/Rdatasets/\">Rdatasets package</a>.\n",
    "\n",
    "prestige = sm.datasets.get_rdataset(\"Duncan\", \"car\", cache=True).data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>income</th>\n",
       "      <th>education</th>\n",
       "      <th>prestige</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accountant</th>\n",
       "      <td>prof</td>\n",
       "      <td>62</td>\n",
       "      <td>86</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pilot</th>\n",
       "      <td>prof</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>architect</th>\n",
       "      <td>prof</td>\n",
       "      <td>75</td>\n",
       "      <td>92</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <td>prof</td>\n",
       "      <td>55</td>\n",
       "      <td>90</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chemist</th>\n",
       "      <td>prof</td>\n",
       "      <td>64</td>\n",
       "      <td>86</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            type  income  education  prestige\n",
       "accountant  prof      62         86        82\n",
       "pilot       prof      72         76        83\n",
       "architect   prof      75         92        90\n",
       "author      prof      55         90        76\n",
       "chemist     prof      64         86        90"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prestige.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "prestige_model = ols(\"prestige ~ income + education\", data=prestige).fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               prestige   R-squared:                       0.828\n",
      "Model:                            OLS   Adj. R-squared:                  0.820\n",
      "Method:                 Least Squares   F-statistic:                     101.2\n",
      "Date:                Wed, 26 Apr 2017   Prob (F-statistic):           8.65e-17\n",
      "Time:                        03:08:15   Log-Likelihood:                -178.98\n",
      "No. Observations:                  45   AIC:                             364.0\n",
      "Df Residuals:                      42   BIC:                             369.4\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -6.0647      4.272     -1.420      0.163       -14.686     2.556\n",
      "income         0.5987      0.120      5.003      0.000         0.357     0.840\n",
      "education      0.5458      0.098      5.555      0.000         0.348     0.744\n",
      "==============================================================================\n",
      "Omnibus:                        1.279   Durbin-Watson:                   1.458\n",
      "Prob(Omnibus):                  0.528   Jarque-Bera (JB):                0.520\n",
      "Skew:                           0.155   Prob(JB):                        0.771\n",
      "Kurtosis:                       3.426   Cond. No.                         163.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(prestige_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Influence plots\n",
    "\n",
    "# Influence plots show the (externally) studentized residuals vs. the leverage of each observation as measured by the hat matrix.\n",
    "#\n",
    "# Externally studentized residuals are residuals that are scaled by their standard deviation where\n",
    "#\n",
    "# $$var(\\\\hat{\\epsilon}_i)=\\hat{\\sigma}^2_i(1-h_{ii})$$\n",
    "#\n",
    "# with\n",
    "#\n",
    "# $$\\hat{\\sigma}^2_i=\\frac{1}{n - p - 1 \\;\\;}\\sum_{j}^{n}\\;\\;\\;\\forall \\;\\;\\; j \\neq i$$\n",
    "#\n",
    "# $n$ is the number of observations and $p$ is the number of regressors. $h_{ii}$ is the $i$-th diagonal element of the hat matrix\n",
    "#\n",
    "# $$H=X(X^{\\;\\prime}X)^{-1}X^{\\;\\prime}$$\n",
    "#\n",
    "# The influence of each point can be visualized by the criterion keyword argument. Options are Cook's distance and DFFITS, two measures of influence.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "fig = sm.graphics.influence_plot(prestige_model, ax=ax, criterion=\"cooks\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# As you can see there are a few worrisome observations. Both contractor and reporter have low leverage but a large residual. <br />\n",
    "# RR.engineer has small residual and large leverage. Conductor and minister have both high leverage and large residuals, and, <br />\n",
    "# therefore, large influence.\n",
    "\n",
    "#### Partial Regression Plots\n",
    "\n",
    "# Since we are doing multivariate regressions, we cannot just look at individual bivariate plots to discern relationships. <br />\n",
    "# Instead, we want to look at the relationship of the dependent variable and independent variables conditional on the other <br />\n",
    "# independent variables. We can do this through using partial regression plots, otherwise known as added variable plots. <br />\n",
    "#\n",
    "# In a partial regression plot, to discern the relationship between the response variable and the $k$-th variabe, we compute <br />\n",
    "# the residuals by regressing the response variable versus the independent variables excluding $X_k$. We can denote this by <br />\n",
    "# $X_{\\sim k}$. We then compute the residuals by regressing $X_k$ on $X_{\\sim k}$. The partial regression plot is the plot <br />\n",
    "# of the former versus the latter residuals. <br />\n",
    "#\n",
    "# The notable points of this plot are that the fitted line has slope $\\beta_k$ and intercept zero. The residuals of this plot <br />\n",
    "# are the same as those of the least squares fit of the original model with full $X$. You can discern the effects of the <br />\n",
    "# individual data values on the estimation of a coefficient easily. If obs_labels is True, then these points are annotated <br />\n",
    "# with their observation label. You can also see the violation of underlying assumptions such as homooskedasticity and <br />\n",
    "# linearity.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "fig = sm.graphics.plot_partregress(\"prestige\", \"income\", [\"income\", \"education\"], data=prestige, ax=ax)\n",
    "ax = fig.axes[0]\n",
    "\n",
    "ax.set_xlim(-2e-15, 1e-14)\n",
    "ax.set_ylim(-25, 30);\n",
    "\n",
    "\n",
    "fix, ax = plt.subplots(figsize=(12,14))\n",
    "fig = sm.graphics.plot_partregress(\"prestige\", \"income\", [\"education\"], data=prestige, ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               prestige   R-squared:                       0.876\n",
      "Model:                            OLS   Adj. R-squared:                  0.870\n",
      "Method:                 Least Squares   F-statistic:                     138.1\n",
      "Date:                Wed, 26 Apr 2017   Prob (F-statistic):           2.02e-18\n",
      "Time:                        03:09:43   Log-Likelihood:                -160.59\n",
      "No. Observations:                  42   AIC:                             327.2\n",
      "Df Residuals:                      39   BIC:                             332.4\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -6.3174      3.680     -1.717      0.094       -13.760     1.125\n",
      "income         0.9307      0.154      6.053      0.000         0.620     1.242\n",
      "education      0.2846      0.121      2.345      0.024         0.039     0.530\n",
      "==============================================================================\n",
      "Omnibus:                        3.811   Durbin-Watson:                   1.468\n",
      "Prob(Omnibus):                  0.149   Jarque-Bera (JB):                2.802\n",
      "Skew:                          -0.614   Prob(JB):                        0.246\n",
      "Kurtosis:                       3.303   Cond. No.                         158.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# As you can see the partial regression plot confirms the influence of conductor, minister, and RR.engineer on the partial relationship between income and prestige. The cases greatly decrease the effect of income on prestige. Dropping these cases confirms this.\n",
    "\n",
    "subset = ~prestige.index.isin([\"conductor\", \"RR.engineer\", \"minister\"])\n",
    "prestige_model2 = ols(\"prestige ~ income + education\", data=prestige, subset=subset).fit()\n",
    "print(prestige_model2.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For a quick check of all the regressors, you can use plot_partregress_grid. These plots will not label the <br />\n",
    "# points, but you can use them to identify problems and then use plot_partregress to get more information.\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "fig = sm.graphics.plot_partregress_grid(prestige_model, fig=fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#### Component-Component plus Residual (CCPR) Plots\n",
    "\n",
    "# The CCPR plot provides a way to judge the effect of one regressor on the <br />\n",
    "# response variable by taking into account the effects of the other  <br />\n",
    "# independent variables. The partial residuals plot is defined as  <br />\n",
    "# $\\text{Residuals} + B_iX_i \\text{ }\\text{ }$   versus $X_i$. The component adds $B_iX_i$ versus  <br />\n",
    "# $X_i$ to show where the fitted line would lie. Care should be taken if $X_i$  <br />\n",
    "# is highly correlated with any of the other independent variables. If this  <br />\n",
    "# is the case, the variance evident in the plot will be an underestimate of  <br />\n",
    "# the true variance.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "fig = sm.graphics.plot_ccpr(prestige_model, \"education\", ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# As you can see the relationship between the variation in prestige explained by education conditional on income seems to be linear, though you can see there are some observations that are exerting considerable influence on the relationship. We can quickly look at more than one variable by using plot_ccpr_grid.\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "fig = sm.graphics.plot_ccpr_grid(prestige_model, fig=fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Regression Plots\n",
    "\n",
    "# The plot_regress_exog function is a convenience function that gives a 2x2 plot containing the dependent variable and fitted values with confidence intervals vs. the independent variable chosen, the residuals of the model vs. the chosen independent variable, a partial regression plot, and a CCPR plot. This function can be used for quickly checking modeling assumptions with respect to a single regressor.\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "fig = sm.graphics.plot_regress_exog(prestige_model, \"education\", fig=fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Fit Plot\n",
    "\n",
    "# The plot_fit function plots the fitted values versus a chosen independent variable. It includes prediction confidence intervals and optionally plots the true dependent variable.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "fig = sm.graphics.plot_fit(prestige_model, \"education\", ax=ax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 murder   R-squared:                       0.813\n",
      "Model:                            OLS   Adj. R-squared:                  0.797\n",
      "Method:                 Least Squares   F-statistic:                     50.08\n",
      "Date:                Wed, 26 Apr 2017   Prob (F-statistic):           3.42e-16\n",
      "Time:                        03:11:29   Log-Likelihood:                -95.050\n",
      "No. Observations:                  51   AIC:                             200.1\n",
      "Df Residuals:                      46   BIC:                             209.8\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -44.1024     12.086     -3.649      0.001       -68.430   -19.774\n",
      "urban          0.0109      0.015      0.707      0.483        -0.020     0.042\n",
      "poverty        0.4121      0.140      2.939      0.005         0.130     0.694\n",
      "hs_grad        0.3059      0.117      2.611      0.012         0.070     0.542\n",
      "single         0.6374      0.070      9.065      0.000         0.496     0.779\n",
      "==============================================================================\n",
      "Omnibus:                        1.618   Durbin-Watson:                   2.507\n",
      "Prob(Omnibus):                  0.445   Jarque-Bera (JB):                0.831\n",
      "Skew:                          -0.220   Prob(JB):                        0.660\n",
      "Kurtosis:                       3.445   Cond. No.                     5.80e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 5.8e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Statewide Crime 2009 Dataset\n",
    "\n",
    "# Compare the following to http://www.ats.ucla.edu/stat/stata/webbooks/reg/chapter4/statareg_self_assessment_answers4.htm\n",
    "#\n",
    "# Though the data here is not the same as in that example. You could run that example by uncommenting the necessary cells below.\n",
    "\n",
    "#dta = pd.read_csv(\"http://www.stat.ufl.edu/~aa/social/csv_files/statewide-crime-2.csv\")\n",
    "#dta = dta.set_index(\"State\", inplace=True).dropna()\n",
    "#dta.rename(columns={\"VR\" : \"crime\",\n",
    "#                    \"MR\" : \"murder\",\n",
    "#                    \"M\"  : \"pctmetro\",\n",
    "#                    \"W\"  : \"pctwhite\",\n",
    "#                    \"H\"  : \"pcths\",\n",
    "#                    \"P\"  : \"poverty\",\n",
    "#                    \"S\"  : \"single\"\n",
    "#                    }, inplace=True)\n",
    "#\n",
    "#crime_model = ols(\"murder ~ pctmetro + poverty + pcths + single\", data=dta).fit()\n",
    "\n",
    "\n",
    "dta = sm.datasets.statecrime.load_pandas().data\n",
    "\n",
    "\n",
    "crime_model = ols(\"murder ~ urban + poverty + hs_grad + single\", data=dta).fit()\n",
    "print(crime_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#### Partial Regression Plots\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "fig = sm.graphics.plot_partregress_grid(crime_model, fig=fig)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "fig = sm.graphics.plot_partregress(\"murder\", \"hs_grad\", [\"urban\", \"poverty\", \"single\"],  ax=ax, data=dta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Leverage-Resid<sup>2</sup> Plot\n",
    "\n",
    "# Closely related to the influence_plot is the leverage-resid<sup>2</sup> plot.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "fig = sm.graphics.plot_leverage_resid2(crime_model, ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Influence Plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "fig = sm.graphics.influence_plot(crime_model, ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Robust linear Model Regression Results                    \n",
      "==============================================================================\n",
      "Dep. Variable:                 murder   No. Observations:                   51\n",
      "Model:                            RLM   Df Residuals:                       46\n",
      "Method:                          IRLS   Df Model:                            4\n",
      "Norm:                   TukeyBiweight                                         \n",
      "Scale Est.:                       mad                                         \n",
      "Cov Type:                          H1                                         \n",
      "Date:                Wed, 26 Apr 2017                                         \n",
      "Time:                        03:13:05                                         \n",
      "No. Iterations:                    50                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -4.2986      9.494     -0.453      0.651       -22.907    14.310\n",
      "urban          0.0029      0.012      0.241      0.809        -0.021     0.027\n",
      "poverty        0.2753      0.110      2.499      0.012         0.059     0.491\n",
      "hs_grad       -0.0302      0.092     -0.328      0.743        -0.211     0.150\n",
      "single         0.2902      0.055      5.253      0.000         0.182     0.398\n",
      "==============================================================================\n",
      "\n",
      "If the model instance has been used for another fit with different fit\n",
      "parameters, then the fit options might not be the correct ones anymore .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### Using robust regression to correct for outliers.\n",
    "\n",
    "# Part of the problem here in recreating the Stata results is that M-estimators are not robust to leverage points. MM-estimators should do better with this examples.\n",
    "\n",
    "from statsmodels.formula.api import rlm\n",
    "\n",
    "\n",
    "rob_crime_model = rlm(\"murder ~ urban + poverty + hs_grad + single\", data=dta,\n",
    "                      M=sm.robust.norms.TukeyBiweight(3)).fit(conv=\"weights\")\n",
    "print(rob_crime_model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saurabh\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:10: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (47,) (51,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e8dd50b5bad7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrob_crime_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mww\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mhat_matrix_diag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mww\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpinv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mresid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrob_crime_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresid\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mresid2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresid\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Saurabh\\Anaconda2\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(left, right, name, na_op)\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mlvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrap_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_na_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m         return construct_result(\n\u001b[1;32m    717\u001b[0m             \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Saurabh\\Anaconda2\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36msafe_na_op\u001b[0;34m(lvalues, rvalues)\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[0mna_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Saurabh\\Anaconda2\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m             result = expressions.evaluate(op, str_rep, x, y,\n\u001b[0;32m--> 652\u001b[0;31m                                           raise_on_error=True, **eval_kwargs)\n\u001b[0m\u001b[1;32m    653\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Saurabh\\Anaconda2\\lib\\site-packages\\pandas\\computation\\expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, op_str, a, b, raise_on_error, use_numexpr, **eval_kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         return _evaluate(op, op_str, a, b, raise_on_error=raise_on_error,\n\u001b[0;32m--> 210\u001b[0;31m                          **eval_kwargs)\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_on_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Saurabh\\Anaconda2\\lib\\site-packages\\pandas\\computation\\expressions.py\u001b[0m in \u001b[0;36m_evaluate_numexpr\u001b[0;34m(op, op_str, a, b, raise_on_error, truediv, reversed, **eval_kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Saurabh\\Anaconda2\\lib\\site-packages\\pandas\\computation\\expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b, raise_on_error, **eval_kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0m_store_test_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (47,) (51,) "
     ]
    }
   ],
   "source": [
    "\n",
    "#rob_crime_model = rlm(\"murder ~ pctmetro + poverty + pcths + single\", data=dta, M=sm.robust.norms.TukeyBiweight()).fit(conv=\"weights\")\n",
    "#print(rob_crime_model.summary())\n",
    "\n",
    "\n",
    "# There aren't yet an influence diagnostics as part of RLM, but we can recreate them. (This depends on the status of [issue #888](https://github.com/statsmodels/statsmodels/issues/808))\n",
    "\n",
    "weights = rob_crime_model.weights\n",
    "idx = weights > 0\n",
    "X = rob_crime_model.model.exog[idx]\n",
    "ww = weights[idx] / weights[idx].mean()\n",
    "hat_matrix_diag = ww*(X*np.linalg.pinv(X).T).sum(1)\n",
    "resid = rob_crime_model.resid\n",
    "resid2 = resid**2\n",
    "resid2 /= resid2.sum()\n",
    "nobs = int(idx.sum())\n",
    "hm = hat_matrix_diag.mean()\n",
    "rm = resid2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resid2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-55c1c220a8e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraphics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresid2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhat_matrix_diag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'o'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m ax = utils.annotate_axes(range(nobs), labels=rob_crime_model.model.data.row_labels[idx],\n\u001b[1;32m      6\u001b[0m                     \u001b[0mpoints\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresid2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhat_matrix_diag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'resid2' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from statsmodels.graphics import utils\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(resid2[idx], hat_matrix_diag, 'o')\n",
    "ax = utils.annotate_axes(range(nobs), labels=rob_crime_model.model.data.row_labels[idx],\n",
    "                    points=lzip(resid2[idx], hat_matrix_diag), offset_points=[(-5,5)]*nobs,\n",
    "                    size=\"large\", ax=ax)\n",
    "ax.set_xlabel(\"resid2\")\n",
    "ax.set_ylabel(\"leverage\")\n",
    "ylim = ax.get_ylim()\n",
    "ax.vlines(rm, *ylim)\n",
    "xlim = ax.get_xlim()\n",
    "ax.hlines(hm, *xlim)\n",
    "ax.margins(0,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
